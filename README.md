# llama-cpp-gpu-docker-
To run local gemma model in a docker container using LLama cpp python
